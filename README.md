# Aether

**Aether** is a powerful, production-ready tokenizer library for building LLMs and AI models.  
It supports **Byte-Level BPE**, **Unigram LM**, **WordPiece**, **SentencePiece models**, **dynamic batching**, and **GPU acceleration** â€” all inspired by leading LLM tokenization strategies.

> âœ¨ The first piece to assembliing the gauntlet (I'm a Marvel guy, not sorry)

---

## ðŸš€ Features

- ðŸ§  Byte-Level BPE (GPT-2/3/4 style)
- ðŸ”¥ Dynamic corpus batching (train on TBs of text)
- ðŸš€ GPU-accelerated token merging (optional)
- ðŸ§© Special tokens: `[PAD]`, `[UNK]`, `[BOS]`, `[EOS]`
- ðŸŽ› Configurable via CLI or Python API
- ðŸ“¦ Pip-installable package
- ðŸŒ Unicode-safe, multilingual tokenization
- ðŸ›  Supports CPU and M1/M2 Mac acceleration

---

## ðŸ“¦ Installation

```bash
pip install aether
```

(or clone and install locally)

```bash
git clone https://github.com/sheltonwlsn/aether.git
cd aether
pip install .
```

---

## ðŸ”¥ Quickstart

**Python API**

```python
from aether.tokenizer.bpe_tokenizer import BPETokenizer

tokenizer = BPETokenizer(vocab_size=8000, byte_level=True)
tokenizer.train(["hello universe ðŸŒŒ"])

encoded = tokenizer.encode("reality is bending")
print(encoded)

decoded = tokenizer.decode(encoded)
print(decoded)
```

**CLI Training**

```bash
aether-cli --corpus my_corpus.txt --model bpe --vocab_size 8000 --use_gpu
```

---

## ðŸ“š Supported Tokenizers

| Type | Description |
|:---|:---|
| BPE | Classic Byte Pair Encoding |
| Unigram | Unigram Language Model |
| WordPiece | Longest-match subword (like BERT) |
| SentencePiece BPE | Raw-text BPE training |
| SentencePiece Unigram | Raw-text Unigram training |

---

## ðŸ›¤ Roadmap

- [ ] Triton GPU accelerated token merging
- [ ] ByteLevel WordPiece
- [ ] Multilingual tokenizer benchmarks
- [ ] Publish to PyPI ðŸŽ¯

---

## ðŸ¤ License

MIT License

---

> _Reality can be whatever you want it to be._  
> â€” Shelton WILSON
